package requiredjava;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.net.URISyntaxException;
import java.text.SimpleDateFormat;
import java.util.Calendar;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.pig.ExecType;
import org.apache.pig.PigServer;

public class NewPigRunner {
	
	public static final String DATE_FORMAT_NOW = "yyyy-MM-dd HH:mm:ss";
	private static final String HADOOP_HOME = "/usr/local/hadoop";
	private static final String DATA_PATH = "/ProjectData";
	private static final String HDFS_INPUT_DIR = "/user/hduser";
	private static final String HDFS_OUTPUT_DIR = "res";
	private static Configuration conf;
	private static FileSystem hdfs;
	private static FileSystem local;
	private static Path inputDir, hdfsFile = new Path(HDFS_INPUT_DIR+"/temp.log");
	private static PigServer pigServer;
	
	@SuppressWarnings("deprecation")
	public static void analyze() throws IOException, URISyntaxException{
		DBConnection.Init();
		
		DBConnection.updateQuery("delete from attack_report_temp");
		DBConnection.updateQuery("alter table attack_report_temp auto_increment=1");
		conf = new Configuration();
		conf.addResource(new Path(HADOOP_HOME+"/etc/hadoop/core-site.xml"));
		conf.addResource(new Path(HADOOP_HOME+"/etc/hadoop/hdfs-site.xml"));
		
		pigServer = new PigServer(ExecType.MAPREDUCE, conf);
		hdfs = FileSystem.get(conf);
		local = FileSystem.getLocal(conf);
		
		File rootDir = new File(DATA_PATH+"/logs");
		File[] logTypesDir = rootDir.listFiles();
		if (logTypesDir != null) {
			for (File child : logTypesDir) {
				if(child.isDirectory())
				{
					inputDir = new Path(child.getPath());
					FileStatus[] inputFiles = local.listStatus(inputDir);
					
					if(inputFiles.length != 0){
						// Merging files & move to hdfs
						BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(hdfs.create(hdfsFile)));
						for(int i=0; i<inputFiles.length; ++i){
							
							Path currentFilePath = inputFiles[i].getPath();
							String currentFileName = currentFilePath.getName(); 
							BufferedReader br = new BufferedReader(new InputStreamReader(local.open(currentFilePath)));
							
							String line;
							while((line = br.readLine()) != null){
								bw.write(currentFileName+"!"+line+"\n");
							}
							br.close();
							
							// delete logs which have been analyzed
							local.delete(currentFilePath);
						}
						bw.close();
						
						// MapReduce Job through Pig
						executePigCommands(hdfsFile.getName(), inputDir.getName()+".xml");
						
						// merge & move result file to local filesystem
						mergePartFiles(new Path(HDFS_INPUT_DIR+"/"+HDFS_OUTPUT_DIR), hdfsFile);
						hdfs.copyToLocalFile(hdfsFile, new Path(DATA_PATH+"/outputs/"+inputDir.getName()));
						
						// delete un-necessary files
						hdfs.delete(hdfsFile);
						hdfs.delete(new Path(HDFS_INPUT_DIR+"/"+HDFS_OUTPUT_DIR));
						
						// split files & get count
						String date = now();
						int[] count = FileTrimOperation.decorate(DATA_PATH+"/outputs/"+inputDir.getName(), hdfsFile.getName(), date, inputFiles);
						
						// make mysql database entry
						for(int i=0; i<count.length; ++i){
							if(count[i]!=0){
								String fileName = inputFiles[i].getPath().getName();
								String type = inputDir.getName();
								DBConnection.updateQuery("INSERT INTO attack_report(FileName,Count,Datetime,Type,OutputLink) VALUES ('" + fileName + "'," + count[i] +",now(),'"+type+"','"+DATA_PATH+"/outputs/"+type+"/"+fileName+"_"+date+"')");
								DBConnection.updateQuery("INSERT INTO attack_report_temp(FileName,Count,Type,OutputLink) VALUES ('" + fileName + "'," + count[i] +",'"+type+"','"+DATA_PATH+"/outputs/"+type+"/"+fileName+"_"+date+"')");
							}
						}
					}
				}
			}
		}
		DBConnection.destroy();
	}
	
	private static void mergePartFiles(Path inputFolder, Path outputFile) throws FileNotFoundException, IOException {
		
		FileStatus[] inputFiles = hdfs.listStatus(inputFolder);
		FSDataOutputStream out = hdfs.create(outputFile);
		
		for(int i=0; i<inputFiles.length; ++i){
			if(!inputFiles[i].getPath().getName().equals("_SUCCESS")){
				FSDataInputStream in = hdfs.open(inputFiles[i].getPath());
				
				byte[] buffer = new byte[256];
				int byteReads = 0;
				
				while((byteReads = in.read(buffer)) > 0){
					out.write(buffer, 0, byteReads);
				}
				in.close();
			}
		}
		out.close();
	}

	public static void main(String args[]) throws IOException, URISyntaxException{
		analyze();
	}
	
	private static void executePigCommands(String logFile, String ruleFile) throws IOException{
		pigServer.registerQuery("REGISTER '"+DATA_PATH+"/PigUDFPython/regexMatching.py"+"' using streaming_python as MyMatch;");
		pigServer.registerQuery("DEFINE XPath org.apache.pig.piggybank.evaluation.xml.XPath();");
		pigServer.registerQuery("temp =  LOAD 'rules/"+ruleFile+"' using org.apache.pig.piggybank.storage.XMLLoader('filter') as (x:chararray);");
		pigServer.registerQuery("regex = FOREACH temp GENERATE XPath(x, 'filter/rule') as rule, XPath(x, 'filter/description') as description;");
		pigServer.registerQuery("logs = LOAD '"+logFile+"' USING PigStorage() AS (line:chararray);");
		pigServer.registerQuery("log = group logs ALL;");
		pigServer.registerQuery("E = foreach regex generate MyMatch.match(log.$1, rule, description);");
		pigServer.store("E", HDFS_OUTPUT_DIR);
	}
	
	public static String now() {
		Calendar cal = Calendar.getInstance();
		SimpleDateFormat sdf = new SimpleDateFormat(DATE_FORMAT_NOW);
		return sdf.format(cal.getTime());

	}
}